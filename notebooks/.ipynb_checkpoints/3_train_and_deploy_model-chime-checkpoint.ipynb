{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Deploying the Fraud Detection Model\n",
    "\n",
    "In this notebook, we will take the outputs from the Processing Job in the previous step and use it and train and deploy an XGBoost model. Our historic transaction dataset is initially comprised of data like timestamp, card number, and transaction amount and we enriched each transaction with features about that card number's recent history, including:\n",
    "\n",
    "- `num_trans_last_10m`\n",
    "- `num_trans_last_1w`\n",
    "- `avg_amt_last_10m`\n",
    "- `avg_amt_last_1w`\n",
    "\n",
    "Individual card numbers may have radically different spending patterns, so we will want to use normalized ratio features to train our XGBoost model to detect fraud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import image_uris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DIR = './data'\n",
    "BUCKET = 'sm-fs-demo'\n",
    "PREFIX = 'training'\n",
    "\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "s3_client = boto3.Session().client('s3')\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "train_feature_group_name = 'cc-transactions-fg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "\n",
    "feature_store_session = sagemaker.Session(boto_session=boto_session, \n",
    "                                          sagemaker_client=sagemaker_client, \n",
    "                                          sagemaker_featurestore_runtime_client=featurestore_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fg = FeatureGroup(name=train_feature_group_name, sagemaker_session=feature_store_session)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query = train_fg.athena_query()\n",
    "train_table = train_query.table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'query_string' (str)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SELECT * FROM \"cc_transactions_fg_1681109844\"'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_string = f'SELECT * FROM \"{train_table}\"'\n",
    "%store query_string\n",
    "query_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Athena query output location: \n",
      "s3://sm-fs-demo/athena-results/query_results/\n"
     ]
    }
   ],
   "source": [
    "query_results= 'athena-results'\n",
    "output_location = f's3://{BUCKET}/{query_results}/query_results/'\n",
    "print(f'Athena query output location: \\n{output_location}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>write_time</th>\n",
       "      <th>api_invocation_time</th>\n",
       "      <th>is_deleted</th>\n",
       "      <th>tid</th>\n",
       "      <th>datetime</th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>amount</th>\n",
       "      <th>amt_ratio1</th>\n",
       "      <th>amt_ratio2</th>\n",
       "      <th>count_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>False</td>\n",
       "      <td>c12d63011963f9c53cb5bf979731a5ad</td>\n",
       "      <td>2020-04-11T00:00:13.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>64.51</td>\n",
       "      <td>0.080569</td>\n",
       "      <td>0.080569</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>False</td>\n",
       "      <td>38cc92a8a842ef1141c2e7a407693129</td>\n",
       "      <td>2020-04-11T00:00:23.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>56.27</td>\n",
       "      <td>0.078967</td>\n",
       "      <td>0.078967</td>\n",
       "      <td>0.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>False</td>\n",
       "      <td>b68b5135950966c6edd9ce23a57f4a1e</td>\n",
       "      <td>2020-04-11T00:02:33.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>95.25</td>\n",
       "      <td>0.544453</td>\n",
       "      <td>0.544453</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>False</td>\n",
       "      <td>2a5d31c3778bb9c56f609c71201cb7ce</td>\n",
       "      <td>2020-04-11T00:04:47.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>88.90</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>False</td>\n",
       "      <td>212fbb2233ca498c4245b78a22ad3a49</td>\n",
       "      <td>2020-04-11T00:04:55.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>746.59</td>\n",
       "      <td>0.503541</td>\n",
       "      <td>0.503541</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    write_time          api_invocation_time  is_deleted  \\\n",
       "0  2023-04-10 06:59:01.744 UTC  2023-04-10 06:59:01.744 UTC       False   \n",
       "1  2023-04-10 06:59:01.744 UTC  2023-04-10 06:59:01.744 UTC       False   \n",
       "2  2023-04-10 06:59:01.744 UTC  2023-04-10 06:59:01.744 UTC       False   \n",
       "3  2023-04-10 06:59:01.744 UTC  2023-04-10 06:59:01.744 UTC       False   \n",
       "4  2023-04-10 06:59:01.744 UTC  2023-04-10 06:59:01.744 UTC       False   \n",
       "\n",
       "                                tid                  datetime  fraud_label  \\\n",
       "0  c12d63011963f9c53cb5bf979731a5ad  2020-04-11T00:00:13.000Z            0   \n",
       "1  38cc92a8a842ef1141c2e7a407693129  2020-04-11T00:00:23.000Z            0   \n",
       "2  b68b5135950966c6edd9ce23a57f4a1e  2020-04-11T00:02:33.000Z            0   \n",
       "3  2a5d31c3778bb9c56f609c71201cb7ce  2020-04-11T00:04:47.000Z            0   \n",
       "4  212fbb2233ca498c4245b78a22ad3a49  2020-04-11T00:04:55.000Z            0   \n",
       "\n",
       "   amount  amt_ratio1  amt_ratio2  count_ratio  \n",
       "0   64.51    0.080569    0.080569     0.043478  \n",
       "1   56.27    0.078967    0.078967     0.035714  \n",
       "2   95.25    0.544453    0.544453     0.041667  \n",
       "3   88.90    0.128571    0.128571     0.055556  \n",
       "4  746.59    0.503541    0.503541     0.040000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_query.run(query_string=query_string, output_location=output_location)\n",
    "train_query.wait()\n",
    "query_df = train_query.as_dataframe()\n",
    "query_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = query_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the results of the SageMaker Processing Job ran in the previous step into a Pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5400000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv(f'{LOCAL_DIR}/aggregated/processing_output.csv')\n",
    "# #df.dropna(inplace=True)\n",
    "# df['cc_num'] = df['cc_num'].astype(np.int64)\n",
    "# df['fraud_label'] = df['fraud_label'].astype(np.int64)\n",
    "# df.head()\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split DataFrame into Train & Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artifically generated dataset contains transactions from `2020-01-01` to `2020-06-01`. We will create a training and validation set out of transactions from `2020-01-15` and `2020-05-15`, discarding the first two weeks in order for our aggregated features to have built up sufficient history for cards and leaving the last two weeks as a holdout test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_start = '2020-01-15'\n",
    "training_end = '2020-05-15'\n",
    "\n",
    "training_df = query_df[(query_df.datetime > training_start) & (query_df.datetime < training_end)]\n",
    "test_df = query_df[query_df.datetime >= training_end]\n",
    "\n",
    "test_df.to_csv(f'{LOCAL_DIR}/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "write_time             4299147\n",
       "api_invocation_time    4299147\n",
       "is_deleted             4299147\n",
       "tid                    4299147\n",
       "datetime               4299147\n",
       "fraud_label            4299147\n",
       "amount                 4299147\n",
       "amt_ratio1             4299147\n",
       "amt_ratio2             4299147\n",
       "count_ratio            4299147\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "write_time             603210\n",
       "api_invocation_time    603210\n",
       "is_deleted             603210\n",
       "tid                    603210\n",
       "datetime               603210\n",
       "fraud_label            603210\n",
       "amount                 603210\n",
       "amt_ratio1             603210\n",
       "amt_ratio2             603210\n",
       "count_ratio            603210\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we now have lots of information about each transaction in our training dataset, we don't want to pass everything as features to the XGBoost algorithm for training because some elements are not useful for detecting fraud or creating a performant model:\n",
    "- A transaction ID and timestamp is unique to the transaction and never seen again. \n",
    "- A card number, if included in the feature set at all, should be a categorical variable. But we don't want our model to learn that specific card numbers are associated with fraud as this might lead to our system blocking genuine behaviour. Instead we should only have the model learn to detect shifting patterns in a card's spending history. \n",
    "- Individual card numbers may have radically different spending patterns, so we will want to use normalized ratio features to train our XGBoost model to detect fraud. \n",
    "\n",
    "Given all of the above, we drop all columns except for the normalised ratio features and transaction amount from our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32310/596874539.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_df.drop(['tid','datetime'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "training_df.drop(['tid','datetime'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [built-in XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) requires the label to be the first column in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>amount</th>\n",
       "      <th>amt_ratio1</th>\n",
       "      <th>amt_ratio2</th>\n",
       "      <th>count_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>64.51</td>\n",
       "      <td>0.080569</td>\n",
       "      <td>0.080569</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>56.27</td>\n",
       "      <td>0.078967</td>\n",
       "      <td>0.078967</td>\n",
       "      <td>0.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>95.25</td>\n",
       "      <td>0.544453</td>\n",
       "      <td>0.544453</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>88.90</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>746.59</td>\n",
       "      <td>0.503541</td>\n",
       "      <td>0.503541</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fraud_label  amount  amt_ratio1  amt_ratio2  count_ratio\n",
       "0            0   64.51    0.080569    0.080569     0.043478\n",
       "1            0   56.27    0.078967    0.078967     0.035714\n",
       "2            0   95.25    0.544453    0.544453     0.041667\n",
       "3            0   88.90    0.128571    0.128571     0.055556\n",
       "4            0  746.59    0.503541    0.503541     0.040000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = training_df[['fraud_label', 'amount', 'amt_ratio1','amt_ratio2','count_ratio']]\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(training_df, test_size=0.3)\n",
    "train.to_csv(f'{LOCAL_DIR}/train.csv', header=False, index=False)\n",
    "val.to_csv(f'{LOCAL_DIR}/val.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/train.csv to s3://sm-fs-demo/training/train.csv        \n",
      "upload: data/val.csv to s3://sm-fs-demo/training/val.csv          \n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {LOCAL_DIR}/train.csv s3://{BUCKET}/{PREFIX}/\n",
    "!aws s3 cp {LOCAL_DIR}/val.csv s3://{BUCKET}/{PREFIX}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2023-04-10-07-07-29-174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-10 07:07:29 Starting - Starting the training job...\n",
      "2023-04-10 07:07:45 Starting - Preparing the instances for training......\n",
      "2023-04-10 07:08:42 Downloading - Downloading input data...\n",
      "2023-04-10 07:09:02 Training - Downloading the training image...\n",
      "2023-04-10 07:09:32 Training - Training image download completed. Training in progress.\u001b[34m[2023-04-10 07:09:49.588 ip-10-2-244-193.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34m[2023-04-10 07:09:50.954 ip-10-2-244-193.ec2.internal:7 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-04-10 07:09:50.955 ip-10-2-244-193.ec2.internal:7 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-04-10 07:09:50.955 ip-10-2-244-193.ec2.internal:7 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-04-10 07:09:50.955 ip-10-2-244-193.ec2.internal:7 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-04-10 07:09:50.955 ip-10-2-244-193.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34mINFO:root:Debug hook created from config\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 3009402 rows and 4 columns\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 1289745 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.00132#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[2023-04-10 07:09:52.259 ip-10-2-244-193.ec2.internal:7 INFO hook.py:413] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2023-04-10 07:09:52.261 ip-10-2-244-193.ec2.internal:7 INFO hook.py:476] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.00131#011validation-error:0.00128\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.00131#011validation-error:0.00128\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.00129#011validation-error:0.00127\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.00129#011validation-error:0.00126\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.00128#011validation-error:0.00125\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.00127#011validation-error:0.00125\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.00126#011validation-error:0.00124\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.00127#011validation-error:0.00124\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.00126#011validation-error:0.00124\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.00125#011validation-error:0.00123\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.00125#011validation-error:0.00123\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.00125#011validation-error:0.00123\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.00125#011validation-error:0.00122\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.00125#011validation-error:0.00123\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.00125#011validation-error:0.00122\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.00124#011validation-error:0.00122\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.00124#011validation-error:0.00122\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.00124#011validation-error:0.00122\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.00124#011validation-error:0.00122\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.00123#011validation-error:0.00121\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.00123#011validation-error:0.00121\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.00123#011validation-error:0.00121\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.00123#011validation-error:0.00121\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.00123#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.00123#011validation-error:0.00121\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.00123#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.00122#011validation-error:0.00121\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.00122#011validation-error:0.00121\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.00122#011validation-error:0.00121\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.00122#011validation-error:0.00121\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.00122#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.00122#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.00122#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.00122#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.00122#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.00122#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.00122#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.00121#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.00121#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.00121#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.00121#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.00121#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.00121#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.00121#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.00121#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.00121#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.00121#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.00121#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.00121#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.00121#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.00121#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.00121#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.00120#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.00120#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.00120#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.00120#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.00120#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.00120#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.00120#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.00120#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.00120#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.00120#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.00120#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.00120#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.00120#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.00120#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.00120#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.00120#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.00120#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.00120#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.00120#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.00120#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.00119#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.00119#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.00119#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.00119#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.00119#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.00119#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.00119#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.00119#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.00119#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.00118#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.00118#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.00118#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.00118#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.00118#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.00118#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.00118#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.00118#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.00118#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.00118#011validation-error:0.00119\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.00118#011validation-error:0.00119\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-04-10 07:11:28 Uploading - Uploading generated training model\n",
      "2023-04-10 07:11:39 Completed - Training job completed\n",
      "Training seconds: 178\n",
      "Billable seconds: 178\n"
     ]
    }
   ],
   "source": [
    "# initialize hyperparameters\n",
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":\"100\"}\n",
    "\n",
    "output_path = 's3://{}/{}/output'.format(BUCKET, PREFIX)\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", sagemaker.Session().boto_region_name, \"1.2-1\")\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.2xlarge', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path)\n",
    "\n",
    "# define the data type and paths to the training and validation datasets\n",
    "content_type = \"csv\"\n",
    "train_input = TrainingInput(\"s3://{}/{}/{}\".format(BUCKET, PREFIX, 'train.csv'), content_type=content_type)\n",
    "validation_input = TrainingInput(\"s3://{}/{}/{}\".format(BUCKET, PREFIX, 'val.csv'), content_type=content_type)\n",
    "\n",
    "# execute the XGBoost training job\n",
    "estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we would perform hyperparameter tuning before deployment, but for the purposes of this example will deploy the model that resulted from the Training Job directly to a SageMaker hosted endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2023-04-10-07-12-24-586\n",
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2023-04-10-07-12-24-586\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2023-04-10-07-12-24-586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.t2.medium',\n",
    "    serializer=sagemaker.serializers.CSVSerializer(), wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'endpoint_name' (str)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sagemaker-xgboost-2023-04-10-07-12-24-586'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name=predictor.endpoint_name\n",
    "#Store the endpoint name for later cleanup \n",
    "#'sagemaker-xgboost-2023-04-03-02-05-40-978'\n",
    "%store endpoint_name\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>write_time</th>\n",
       "      <th>api_invocation_time</th>\n",
       "      <th>is_deleted</th>\n",
       "      <th>tid</th>\n",
       "      <th>datetime</th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>amount</th>\n",
       "      <th>amt_ratio1</th>\n",
       "      <th>amt_ratio2</th>\n",
       "      <th>count_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2959</th>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>False</td>\n",
       "      <td>349e05e3a7d808953461c06879952b8c</td>\n",
       "      <td>2020-05-20T00:06:22.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>28.14</td>\n",
       "      <td>0.034184</td>\n",
       "      <td>0.034184</td>\n",
       "      <td>0.035714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2960</th>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>False</td>\n",
       "      <td>a61687f327cecb9108daa5e9b9931150</td>\n",
       "      <td>2020-05-20T00:09:17.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>87.40</td>\n",
       "      <td>0.174536</td>\n",
       "      <td>0.174536</td>\n",
       "      <td>0.027778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2961</th>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>False</td>\n",
       "      <td>f59d9b46551678557e47e550d56232b8</td>\n",
       "      <td>2020-05-20T00:16:35.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>593.38</td>\n",
       "      <td>1.013424</td>\n",
       "      <td>1.013424</td>\n",
       "      <td>0.038462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>False</td>\n",
       "      <td>dfc59cd12a403a8f3a0af712af96fb94</td>\n",
       "      <td>2020-05-20T00:17:40.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>893.78</td>\n",
       "      <td>1.319359</td>\n",
       "      <td>1.319359</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2963</th>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>2023-04-10 06:59:01.744 UTC</td>\n",
       "      <td>False</td>\n",
       "      <td>047b6648cfe47787c68ba5a97da03b75</td>\n",
       "      <td>2020-05-20T00:17:40.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>3279.28</td>\n",
       "      <td>9.499276</td>\n",
       "      <td>9.499276</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       write_time          api_invocation_time  is_deleted  \\\n",
       "2959  2023-04-10 06:59:01.744 UTC  2023-04-10 06:59:01.744 UTC       False   \n",
       "2960  2023-04-10 06:59:01.744 UTC  2023-04-10 06:59:01.744 UTC       False   \n",
       "2961  2023-04-10 06:59:01.744 UTC  2023-04-10 06:59:01.744 UTC       False   \n",
       "2962  2023-04-10 06:59:01.744 UTC  2023-04-10 06:59:01.744 UTC       False   \n",
       "2963  2023-04-10 06:59:01.744 UTC  2023-04-10 06:59:01.744 UTC       False   \n",
       "\n",
       "                                   tid                  datetime  fraud_label  \\\n",
       "2959  349e05e3a7d808953461c06879952b8c  2020-05-20T00:06:22.000Z            0   \n",
       "2960  a61687f327cecb9108daa5e9b9931150  2020-05-20T00:09:17.000Z            0   \n",
       "2961  f59d9b46551678557e47e550d56232b8  2020-05-20T00:16:35.000Z            0   \n",
       "2962  dfc59cd12a403a8f3a0af712af96fb94  2020-05-20T00:17:40.000Z            0   \n",
       "2963  047b6648cfe47787c68ba5a97da03b75  2020-05-20T00:17:40.000Z            0   \n",
       "\n",
       "       amount  amt_ratio1  amt_ratio2  count_ratio  \n",
       "2959    28.14    0.034184    0.034184     0.035714  \n",
       "2960    87.40    0.174536    0.174536     0.027778  \n",
       "2961   593.38    1.013424    1.013424     0.038462  \n",
       "2962   893.78    1.319359    1.319359     0.050000  \n",
       "2963  3279.28    9.499276    9.499276     0.052632  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to check that our endpoint is working, let's call it directly with a record from our test hold-out set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'28.14,0.0341843260231609,0.0341843260231609,0.0357142857142857'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload_df = test_df.drop(['tid','datetime','write_time','api_invocation_time','is_deleted','fraud_label'], axis=1)\n",
    "payload = payload_df.head(1).to_csv(index=False, header=False).strip()\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003757273661904037"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(predictor.predict(payload).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show that the model predicts FRAUD / NOT FRAUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With transaction count ratio of: 0.30, fraud score: 0.971\n"
     ]
    }
   ],
   "source": [
    "count_ratio = 0.30\n",
    "payload = f'1.00,1.0,1.0,{count_ratio:.2f}'\n",
    "is_fraud = float(predictor.predict(payload).decode('utf-8'))\n",
    "print(f'With transaction count ratio of: {count_ratio:.2f}, fraud score: {is_fraud:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With transaction count ratio of: 0.06, fraud score: 0.002\n"
     ]
    }
   ],
   "source": [
    "count_ratio = 0.06\n",
    "payload = f'1.00,1.0,1.0,{count_ratio:.2f}'\n",
    "is_fraud = float(predictor.predict(payload).decode('utf-8'))\n",
    "print(f'With transaction count ratio of: {count_ratio:.2f}, fraud score: {is_fraud:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
