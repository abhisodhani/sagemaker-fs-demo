{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Deploying the Fraud Detection Model\n",
    "\n",
    "In this notebook, we will take the outputs from the Processing Job in the previous step and use it and train and deploy an XGBoost model. Our historic transaction dataset is initially comprised of data like timestamp, card number, and transaction amount and we enriched each transaction with features about that card number's recent history, including:\n",
    "\n",
    "- `num_trans_last_10m`\n",
    "- `num_trans_last_1w`\n",
    "- `avg_amt_last_10m`\n",
    "- `avg_amt_last_1w`\n",
    "\n",
    "Individual card numbers may have radically different spending patterns, so we will want to use normalized ratio features to train our XGBoost model to detect fraud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import image_uris\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_DIR = './data'\n",
    "BUCKET = 'chime-fs-demo'\n",
    "PREFIX = 'training'\n",
    "\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "s3_client = boto3.Session().client('s3')\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "train_feature_group_name = 'cc_train_fg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "\n",
    "feature_store_session = sagemaker.Session(boto_session=boto_session, \n",
    "                                          sagemaker_client=sagemaker_client, \n",
    "                                          sagemaker_featurestore_runtime_client=featurestore_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fg = FeatureGroup(name=train_feature_group_name, sagemaker_session=feature_store_session)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query = train_fg.athena_query()\n",
    "train_table = train_query.table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'query_string' (str)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SELECT * FROM \"cc_train_fg_1680709766\"'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_string = f'SELECT * FROM \"{train_table}\"'\n",
    "%store query_string\n",
    "query_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Athena query output location: \n",
      "s3://chime-fs-demo/sagemaker-fs-demo/query_results/\n"
     ]
    }
   ],
   "source": [
    "query_results= 'sagemaker-fs-demo'\n",
    "output_location = f's3://{BUCKET}/{query_results}/query_results/'\n",
    "print(f'Athena query output location: \\n{output_location}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>write_time</th>\n",
       "      <th>api_invocation_time</th>\n",
       "      <th>is_deleted</th>\n",
       "      <th>tid</th>\n",
       "      <th>datetime</th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>amount</th>\n",
       "      <th>amt_ratio1</th>\n",
       "      <th>amt_ratio2</th>\n",
       "      <th>count_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-05 15:50:02.164 UTC</td>\n",
       "      <td>2023-04-05 15:50:02.164 UTC</td>\n",
       "      <td>False</td>\n",
       "      <td>01d85107fb32540768c2403c5d1da0c7</td>\n",
       "      <td>2020-03-01T00:05:50.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>804.40</td>\n",
       "      <td>1.705822</td>\n",
       "      <td>1.705822</td>\n",
       "      <td>0.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-05 15:50:02.164 UTC</td>\n",
       "      <td>2023-04-05 15:50:02.164 UTC</td>\n",
       "      <td>False</td>\n",
       "      <td>dc090421a69f9a7aaed7ff4282509a9d</td>\n",
       "      <td>2020-03-01T00:11:12.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>58.14</td>\n",
       "      <td>0.085644</td>\n",
       "      <td>0.085644</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-05 15:50:02.164 UTC</td>\n",
       "      <td>2023-04-05 15:50:02.164 UTC</td>\n",
       "      <td>False</td>\n",
       "      <td>4b260f55d100b9ce29416db4a9dc6aff</td>\n",
       "      <td>2020-03-01T00:11:25.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>4.56</td>\n",
       "      <td>0.048174</td>\n",
       "      <td>0.007007</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-05 15:50:02.164 UTC</td>\n",
       "      <td>2023-04-05 15:50:02.164 UTC</td>\n",
       "      <td>False</td>\n",
       "      <td>f81ed0282ddf81d523c2ac3fbd498d79</td>\n",
       "      <td>2020-03-01T00:13:27.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>295.96</td>\n",
       "      <td>0.821689</td>\n",
       "      <td>0.821689</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-04-05 15:50:02.164 UTC</td>\n",
       "      <td>2023-04-05 15:50:02.164 UTC</td>\n",
       "      <td>False</td>\n",
       "      <td>a0f38e7ca03665ca7a0b85218bcf9c92</td>\n",
       "      <td>2020-03-01T00:14:04.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>486.50</td>\n",
       "      <td>0.602198</td>\n",
       "      <td>0.602198</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    write_time          api_invocation_time  is_deleted  \\\n",
       "0  2023-04-05 15:50:02.164 UTC  2023-04-05 15:50:02.164 UTC       False   \n",
       "1  2023-04-05 15:50:02.164 UTC  2023-04-05 15:50:02.164 UTC       False   \n",
       "2  2023-04-05 15:50:02.164 UTC  2023-04-05 15:50:02.164 UTC       False   \n",
       "3  2023-04-05 15:50:02.164 UTC  2023-04-05 15:50:02.164 UTC       False   \n",
       "4  2023-04-05 15:50:02.164 UTC  2023-04-05 15:50:02.164 UTC       False   \n",
       "\n",
       "                                tid                  datetime  fraud_label  \\\n",
       "0  01d85107fb32540768c2403c5d1da0c7  2020-03-01T00:05:50.000Z            0   \n",
       "1  dc090421a69f9a7aaed7ff4282509a9d  2020-03-01T00:11:12.000Z            0   \n",
       "2  4b260f55d100b9ce29416db4a9dc6aff  2020-03-01T00:11:25.000Z            0   \n",
       "3  f81ed0282ddf81d523c2ac3fbd498d79  2020-03-01T00:13:27.000Z            0   \n",
       "4  a0f38e7ca03665ca7a0b85218bcf9c92  2020-03-01T00:14:04.000Z            0   \n",
       "\n",
       "   amount  amt_ratio1  amt_ratio2  count_ratio  \n",
       "0  804.40    1.705822    1.705822     0.034483  \n",
       "1   58.14    0.085644    0.085644     0.043478  \n",
       "2    4.56    0.048174    0.007007     0.083333  \n",
       "3  295.96    0.821689    0.821689     0.037037  \n",
       "4  486.50    0.602198    0.602198     0.041667  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_query.run(query_string=query_string, output_location=output_location)\n",
    "train_query.wait()\n",
    "query_df = train_query.as_dataframe()\n",
    "query_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['write_time', 'api_invocation_time', 'is_deleted', 'tid', 'datetime',\n",
       "       'fraud_label', 'amount', 'amt_ratio1', 'amt_ratio2', 'count_ratio'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = query_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the results of the SageMaker Processing Job ran in the previous step into a Pandas dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5400000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv(f'{LOCAL_DIR}/aggregated/processing_output.csv')\n",
    "# #df.dropna(inplace=True)\n",
    "# df['cc_num'] = df['cc_num'].astype(np.int64)\n",
    "# df['fraud_label'] = df['fraud_label'].astype(np.int64)\n",
    "# df.head()\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split DataFrame into Train & Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artifically generated dataset contains transactions from `2020-01-01` to `2020-06-01`. We will create a training and validation set out of transactions from `2020-01-15` and `2020-05-15`, discarding the first two weeks in order for our aggregated features to have built up sufficient history for cards and leaving the last two weeks as a holdout test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_start = '2020-01-15'\n",
    "training_end = '2020-05-15'\n",
    "\n",
    "training_df = query_df[(query_df.datetime > training_start) & (query_df.datetime < training_end)]\n",
    "test_df = query_df[query_df.datetime >= training_end]\n",
    "\n",
    "test_df.to_csv(f'{LOCAL_DIR}/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "write_time             4299147\n",
       "api_invocation_time    4299147\n",
       "is_deleted             4299147\n",
       "tid                    4299147\n",
       "datetime               4299147\n",
       "fraud_label            4299147\n",
       "amount                 4299147\n",
       "amt_ratio1             4299147\n",
       "amt_ratio2             4299147\n",
       "count_ratio            4299147\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "write_time             603210\n",
       "api_invocation_time    603210\n",
       "is_deleted             603210\n",
       "tid                    603210\n",
       "datetime               603210\n",
       "fraud_label            603210\n",
       "amount                 603210\n",
       "amt_ratio1             603210\n",
       "amt_ratio2             603210\n",
       "count_ratio            603210\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we now have lots of information about each transaction in our training dataset, we don't want to pass everything as features to the XGBoost algorithm for training because some elements are not useful for detecting fraud or creating a performant model:\n",
    "- A transaction ID and timestamp is unique to the transaction and never seen again. \n",
    "- A card number, if included in the feature set at all, should be a categorical variable. But we don't want our model to learn that specific card numbers are associated with fraud as this might lead to our system blocking genuine behaviour. Instead we should only have the model learn to detect shifting patterns in a card's spending history. \n",
    "- Individual card numbers may have radically different spending patterns, so we will want to use normalized ratio features to train our XGBoost model to detect fraud. \n",
    "\n",
    "Given all of the above, we drop all columns except for the normalised ratio features and transaction amount from our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13395/3217280059.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_df.drop(['tid','datetime','cc_num','num_trans_last_10m', 'avg_amt_last_10m',\n"
     ]
    }
   ],
   "source": [
    "# training_df.drop(['tid','datetime','cc_num','num_trans_last_10m', 'avg_amt_last_10m',\n",
    "#        'num_trans_last_1w', 'avg_amt_last_1w'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16325/596874539.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training_df.drop(['tid','datetime'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "training_df.drop(['tid','datetime'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [built-in XGBoost algorithm](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) requires the label to be the first column in the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>amount</th>\n",
       "      <th>amt_ratio1</th>\n",
       "      <th>amt_ratio2</th>\n",
       "      <th>count_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>804.40</td>\n",
       "      <td>1.705822</td>\n",
       "      <td>1.705822</td>\n",
       "      <td>0.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>58.14</td>\n",
       "      <td>0.085644</td>\n",
       "      <td>0.085644</td>\n",
       "      <td>0.043478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4.56</td>\n",
       "      <td>0.048174</td>\n",
       "      <td>0.007007</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>295.96</td>\n",
       "      <td>0.821689</td>\n",
       "      <td>0.821689</td>\n",
       "      <td>0.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>486.50</td>\n",
       "      <td>0.602198</td>\n",
       "      <td>0.602198</td>\n",
       "      <td>0.041667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fraud_label  amount  amt_ratio1  amt_ratio2  count_ratio\n",
       "0            0  804.40    1.705822    1.705822     0.034483\n",
       "1            0   58.14    0.085644    0.085644     0.043478\n",
       "2            0    4.56    0.048174    0.007007     0.083333\n",
       "3            0  295.96    0.821689    0.821689     0.037037\n",
       "4            0  486.50    0.602198    0.602198     0.041667"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_df = training_df[['fraud_label', 'amount', 'amt_ratio1','amt_ratio2','count_ratio']]\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(training_df, test_size=0.3)\n",
    "train.to_csv(f'{LOCAL_DIR}/train.csv', header=False, index=False)\n",
    "val.to_csv(f'{LOCAL_DIR}/val.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/train.csv to s3://chime-fs-demo/training/train.csv     \n",
      "upload: data/val.csv to s3://chime-fs-demo/training/val.csv      \n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {LOCAL_DIR}/train.csv s3://{BUCKET}/{PREFIX}/\n",
    "!aws s3 cp {LOCAL_DIR}/val.csv s3://{BUCKET}/{PREFIX}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-05 19:51:47 Starting - Starting the training job...ProfilerReport-1680724306: InProgress\n",
      "...\n",
      "2023-04-05 19:52:47 Starting - Preparing the instances for training...\n",
      "2023-04-05 19:53:11 Downloading - Downloading input data...\n",
      "2023-04-05 19:53:47 Training - Downloading the training image...\n",
      "2023-04-05 19:54:07 Training - Training image download completed. Training in progress..\u001b[34m[2023-04-05 19:54:16.142 ip-10-0-242-183.ec2.internal:7 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 3009402 rows and 4 columns\u001b[0m\n",
      "\u001b[34mINFO:root:Validation matrix has 1289745 rows\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.00131#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.00130#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.00131#011validation-error:0.00130\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.00129#011validation-error:0.00129\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.00129#011validation-error:0.00129\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.00127#011validation-error:0.00127\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.00128#011validation-error:0.00127\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.00127#011validation-error:0.00126\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.00127#011validation-error:0.00126\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.00127#011validation-error:0.00127\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.00127#011validation-error:0.00126\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.00127#011validation-error:0.00126\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.00127#011validation-error:0.00127\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.00126#011validation-error:0.00127\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.00126#011validation-error:0.00126\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.00126#011validation-error:0.00126\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.00125#011validation-error:0.00124\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.00125#011validation-error:0.00125\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.00125#011validation-error:0.00126\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.00125#011validation-error:0.00125\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.00125#011validation-error:0.00125\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.00125#011validation-error:0.00125\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.00125#011validation-error:0.00124\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.00124#011validation-error:0.00124\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.00124#011validation-error:0.00125\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.00123#011validation-error:0.00124\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.00122#011validation-error:0.00124\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.00123#011validation-error:0.00124\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.00123#011validation-error:0.00124\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.00122#011validation-error:0.00124\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.00123#011validation-error:0.00124\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.00123#011validation-error:0.00124\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.00122#011validation-error:0.00123\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.00123#011validation-error:0.00123\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.00123#011validation-error:0.00124\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.00122#011validation-error:0.00123\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.00122#011validation-error:0.00123\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.00122#011validation-error:0.00123\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.00122#011validation-error:0.00123\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.00122#011validation-error:0.00123\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.00122#011validation-error:0.00123\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.00122#011validation-error:0.00123\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.00122#011validation-error:0.00123\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.00122#011validation-error:0.00123\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.00122#011validation-error:0.00123\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.00121#011validation-error:0.00122\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.00121#011validation-error:0.00122\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.00121#011validation-error:0.00122\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.00121#011validation-error:0.00122\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.00121#011validation-error:0.00121\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.00121#011validation-error:0.00121\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.00121#011validation-error:0.00121\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.00121#011validation-error:0.00121\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.00120#011validation-error:0.00121\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.00121#011validation-error:0.00122\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.00120#011validation-error:0.00122\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.00120#011validation-error:0.00121\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.00120#011validation-error:0.00121\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.00120#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.00119#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.00119#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.00119#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.00119#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.00119#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.00119#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.00119#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.00119#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.00119#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.00119#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.00119#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\n",
      "2023-04-05 19:55:47 Uploading - Uploading generated training model\u001b[34m[97]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.00118#011validation-error:0.00120\u001b[0m\n",
      "\n",
      "2023-04-05 19:56:08 Completed - Training job completed\n",
      "Training seconds: 168\n",
      "Billable seconds: 168\n"
     ]
    }
   ],
   "source": [
    "# initialize hyperparameters\n",
    "hyperparameters = {\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"binary:logistic\",\n",
    "        \"num_round\":\"100\"}\n",
    "\n",
    "output_path = 's3://{}/{}/output'.format(BUCKET, PREFIX)\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", sagemaker.Session().boto_region_name, \"1.2-1\")\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.2xlarge', \n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path)\n",
    "\n",
    "# define the data type and paths to the training and validation datasets\n",
    "content_type = \"csv\"\n",
    "train_input = TrainingInput(\"s3://{}/{}/{}\".format(BUCKET, PREFIX, 'train.csv'), content_type=content_type)\n",
    "validation_input = TrainingInput(\"s3://{}/{}/{}\".format(BUCKET, PREFIX, 'val.csv'), content_type=content_type)\n",
    "\n",
    "# execute the XGBoost training job\n",
    "estimator.fit({'train': train_input, 'validation': validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we would perform hyperparameter tuning before deployment, but for the purposes of this example will deploy the model that resulted from the Training Job directly to a SageMaker hosted endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type='ml.t2.medium',\n",
    "    serializer=sagemaker.serializers.CSVSerializer(), wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'endpoint_name' (str)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sagemaker-xgboost-2023-04-03-02-05-40-978'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name=predictor.endpoint_name\n",
    "#Store the endpoint name for later cleanup \n",
    "%store endpoint_name\n",
    "endpoint_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to check that our endpoint is working, let's call it directly with a record from our test hold-out set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'485.75,0.4152517518202095,0.4152517518202095,0.0384615384615384'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload_df = test_df.drop(['tid','datetime','cc_num','fraud_label','num_trans_last_10m', 'avg_amt_last_10m',\n",
    "       'num_trans_last_1w', 'avg_amt_last_1w'], axis=1)\n",
    "payload = payload_df.head(1).to_csv(index=False, header=False).strip()\n",
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002807091223075986"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(predictor.predict(payload).decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show that the model predicts FRAUD / NOT FRAUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With transaction count ratio of: 0.30, fraud score: 0.978\n"
     ]
    }
   ],
   "source": [
    "count_ratio = 0.30\n",
    "payload = f'1.00,1.0,1.0,{count_ratio:.2f}'\n",
    "is_fraud = float(predictor.predict(payload).decode('utf-8'))\n",
    "print(f'With transaction count ratio of: {count_ratio:.2f}, fraud score: {is_fraud:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With transaction count ratio of: 0.06, fraud score: 0.004\n"
     ]
    }
   ],
   "source": [
    "count_ratio = 0.06\n",
    "payload = f'1.00,1.0,1.0,{count_ratio:.2f}'\n",
    "is_fraud = float(predictor.predict(payload).decode('utf-8'))\n",
    "print(f'With transaction count ratio of: {count_ratio:.2f}, fraud score: {is_fraud:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
