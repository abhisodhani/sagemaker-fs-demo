{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Batch Ingestion\n",
    "**This notebook aggregates raw features into new derived features that is used for Fraud Detection model training/inference.**\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Create PySpark Processing Script](#Create-PySpark-Processing-Script)\n",
    "1. [Run SageMaker Processing Job](#Run-SageMaker-Processing-Job)\n",
    "1. [Explore Aggregated Features](#Explore-Aggregated-Features)\n",
    "1. [Validate Feature Group for Records](#Validate-Feature-Group-for-Records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Background\n",
    "\n",
    "- This notebook takes raw credit card transactions data (csv) generated by \n",
    "[notebook 0](./0_prepare_transactions_dataset.ipynb) and aggregates the raw features to create new features (ratios) via <b>SageMaker Processing</b> PySpark Job. These aggregated features alongside the raw original features will be leveraged in the training phase of a Credit Card Fraud Detection model in the next step (see notebook [notebook 3](./3_train_and_deploy_model.ipynb)).\n",
    "\n",
    "- As part of the Spark job, we also select the latest weekly aggregated features - `num_trans_last_1w` and `avg_amt_last_1w` grouped by `cc_num` (credit card number) and populate these features into the <b>SageMaker Online Feature Store</b> as a feature group. This feature group (`cc-agg-batch-fg`) was created in notebook [notebook 1](./1_setup.ipynb).\n",
    "\n",
    "- [Amazon SageMaker Processing](https://aws.amazon.com/about-aws/whats-new/2020/09/amazon-sagemaker-processing-now-supports-built-in-spark-containers-for-big-data-processing/) lets customers run analytics jobs for data engineering and model evaluation on Amazon SageMaker easily and at scale. It provides a fully managed Spark environment for data processing or feature engineering workloads.\n",
    "\n",
    "<img src=\"./images/batch_ingestion.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import logging\n",
    "import random\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker version: 2.48.1\n"
     ]
    }
   ],
   "source": [
    "print(f'Using SageMaker version: {sagemaker.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Setup Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Batch Aggregation using SageMaker PySpark Processing Job]\n"
     ]
    }
   ],
   "source": [
    "logger.info('[Batch Aggregation using SageMaker PySpark Processing Job]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "BUCKET = 'chime-fs-demo'\n",
    "INPUT_KEY_PREFIX = 'raw'\n",
    "OUTPUT_KEY_PREFIX = 'aggregated'\n",
    "LOCAL_DIR = './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Create PySpark Script\n",
    "This PySpark script does the following:\n",
    "\n",
    "1. Aggregates raw features to derive new features (ratios).\n",
    "2. Saves the aggregated features alongside the original raw features into a CSV file and writes it to S3 - will be used in the next step for model training.\n",
    "3. Groups the aggregated features by credit card number and picks selected aggregated features to write to SageMaker Feature Store (Online). <br>\n",
    "<b>Note: </b> The feature group was created in the previous notebook (`1_setup.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting batch_aggregation_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile batch_aggregation_train1.py\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, TimestampType, LongType\n",
    "from pyspark.sql.functions import desc, dense_rank\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from  argparse import Namespace, ArgumentParser\n",
    "from pyspark.sql.window import Window\n",
    "import argparse\n",
    "import logging\n",
    "import boto3\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "TOTAL_UNIQUE_USERS = 10000\n",
    "FEATURE_GROUP = 'cc-agg-batch-chime-fg'\n",
    "TRAIN_FEATURE_GROUP = 'cc-train-chime-fg'\n",
    "\n",
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "feature_store_client = boto3.client(service_name='sagemaker-featurestore-runtime')\n",
    "\n",
    "\n",
    "def parse_args() -> Namespace:\n",
    "    parser = ArgumentParser(description='Spark Job Input and Output Args')\n",
    "    parser.add_argument('--s3_input_bucket', type=str, help='S3 Input Bucket')\n",
    "    parser.add_argument('--s3_input_key_prefix', type=str, help='S3 Input Key Prefix')\n",
    "    parser.add_argument('--s3_output_bucket', type=str, help='S3 Output Bucket')\n",
    "    parser.add_argument('--s3_output_key_prefix', type=str, help='S3 Output Key Prefix')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "    \n",
    "\n",
    "def define_schema() -> StructType:\n",
    "    schema = StructType([StructField('tid', StringType(), True),\n",
    "                         StructField('datetime', TimestampType(), True),\n",
    "                         StructField('cc_num', LongType(), True),\n",
    "                         StructField('amount', DoubleType(), True),\n",
    "                         StructField('fraud_label', StringType(), True)])\n",
    "    return schema\n",
    "\n",
    "\n",
    "def aggregate_features(args: Namespace, schema: StructType, spark: SparkSession) -> DataFrame:\n",
    "    logger.info('[Read Raw Transactions Data as Spark DataFrame]')\n",
    "    transactions_df = spark.read.csv(f's3a://{os.path.join(args.s3_input_bucket, args.s3_input_key_prefix)}', \\\n",
    "                                     header=False, \\\n",
    "                                     schema=schema)\n",
    "    logger.info('[Aggregate Transactions to Derive New Features using Spark SQL]')\n",
    "    query = \"\"\"\n",
    "    SELECT *, \\\n",
    "           avg_amt_last_10m/avg_amt_last_1w AS amt_ratio1, \\\n",
    "           amount/avg_amt_last_1w AS amt_ratio2, \\\n",
    "           num_trans_last_10m/num_trans_last_1w AS count_ratio \\\n",
    "    FROM \\\n",
    "        ( \\\n",
    "        SELECT *, \\\n",
    "               COUNT(*) OVER w1 as num_trans_last_10m, \\\n",
    "               AVG(amount) OVER w1 as avg_amt_last_10m, \\\n",
    "               COUNT(*) OVER w2 as num_trans_last_1w, \\\n",
    "               AVG(amount) OVER w2 as avg_amt_last_1w \\\n",
    "        FROM transactions_df \\\n",
    "        WINDOW \\\n",
    "               w1 AS (PARTITION BY cc_num order by cast(datetime AS timestamp) RANGE INTERVAL 10 MINUTE PRECEDING), \\\n",
    "               w2 AS (PARTITION BY cc_num order by cast(datetime AS timestamp) RANGE INTERVAL 1 WEEK PRECEDING) \\\n",
    "        ) \n",
    "    \"\"\"\n",
    "    transactions_df.registerTempTable('transactions_df')\n",
    "    aggregated_features = spark.sql(query)\n",
    "    return aggregated_features\n",
    "\n",
    "\n",
    "def write_to_s3(args: Namespace, aggregated_features: DataFrame) -> None:\n",
    "    logger.info('[Write Aggregated Features to S3]')\n",
    "    aggregated_features.coalesce(1) \\\n",
    "                       .write.format('com.databricks.spark.csv') \\\n",
    "                       .option('header', True) \\\n",
    "                       .mode('overwrite') \\\n",
    "                       .option('sep', ',') \\\n",
    "                       .save('s3a://' + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix))\n",
    "    \n",
    "def group_by_card_number(aggregated_features: DataFrame) -> DataFrame: \n",
    "    logger.info('[Group Aggregated Features by Card Number]')\n",
    "    window = Window.partitionBy('cc_num').orderBy(desc('datetime'))\n",
    "    sorted_df = aggregated_features.withColumn('rank', dense_rank().over(window))\n",
    "    grouped_df = sorted_df.filter(sorted_df.rank == 1).drop(sorted_df.rank)\n",
    "    sliced_df = grouped_df.select('cc_num', 'num_trans_last_1w', 'avg_amt_last_1w')\n",
    "    return sliced_df\n",
    "\n",
    "\n",
    "def transform_row(sliced_df: DataFrame) -> list:\n",
    "    logger.info('[Transform Spark DataFrame Row to SageMaker Feature Store Record]')\n",
    "    records = []\n",
    "    for row in sliced_df.rdd.collect():\n",
    "        record = []\n",
    "        cc_num, num_trans_last_1w, avg_amt_last_1w = row\n",
    "        if cc_num:\n",
    "            record.append({'ValueAsString': str(cc_num), 'FeatureName': 'cc_num'})\n",
    "            record.append({'ValueAsString': str(num_trans_last_1w), 'FeatureName': 'num_trans_last_1w'})\n",
    "            record.append({'ValueAsString': str(round(avg_amt_last_1w, 2)), 'FeatureName': 'avg_amt_last_1w'})\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "\n",
    "def write_to_feature_store(records: list) -> None:\n",
    "    logger.info('[Write Grouped Features to SageMaker Feature Store]')\n",
    "    success, fail = 0, 0\n",
    "    for record in records:\n",
    "        event_time_feature = {\n",
    "                'FeatureName': 'trans_time',\n",
    "                'ValueAsString': str(int(round(time.time())))\n",
    "            }\n",
    "        record.append(event_time_feature)\n",
    "        response = feature_store_client.put_record(FeatureGroupName=FEATURE_GROUP, Record=record)\n",
    "        if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "    logger.info('Success = {}'.format(success))\n",
    "    logger.info('Fail = {}'.format(fail))\n",
    "    assert success == TOTAL_UNIQUE_USERS\n",
    "    assert fail == 0\n",
    "\n",
    "def transform_row_train(df: DataFrame) -> list:\n",
    "    logger.info('[Transform Spark DataFrame Row to SageMaker Feature Store Record]')\n",
    "    records = []\n",
    "    for row in df.rdd.collect():\n",
    "        record = []\n",
    "        tid,cc_num, amount, fraud_label, num_trans_last_10m, avg_amt_last_10m, num_trans_last_1w, avg_amt_last_1w,datetime, amt_ratio1,amt_ratio2,count_ratio = row\n",
    "        if tid:\n",
    "            record.append({'ValueAsString': str(tid), 'FeatureName': 'tid'})\n",
    "            record.append({'ValueAsString': str(cc_num), 'FeatureName': 'cc_num'})\n",
    "            record.append({'ValueAsString': str(round(amount, 2)), 'FeatureName': 'amount'})\n",
    "            record.append({'ValueAsString': str(fraud_label), 'FeatureName': 'fraud_label'})\n",
    "            record.append({'ValueAsString': str(num_trans_last_1w), 'FeatureName': 'num_trans_last_1w'})\n",
    "            record.append({'ValueAsString': str(round(avg_amt_last_1w, 2)), 'FeatureName': 'avg_amt_last_1w'})\n",
    "            record.append({'ValueAsString': str(datetime), 'FeatureName': 'datetime'})\n",
    "            record.append({'ValueAsString': str(num_trans_last_10m), 'FeatureName': 'num_trans_last_10m'})\n",
    "            record.append({'ValueAsString': str(round(avg_amt_last_10m, 2)), 'FeatureName': 'avg_amt_last_10m'})\n",
    "            record.append({'ValueAsString': str(round(amt_ratio1, 2)), 'FeatureName': 'amt_ratio1'})\n",
    "            record.append({'ValueAsString': str(round(amt_ratio2, 2)), 'FeatureName': 'amt_ratio2'})\n",
    "            record.append({'ValueAsString': str(round(count_ratio, 2)), 'FeatureName': 'count_ratio'})\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "def write_to_train_feature_store(records: list) -> None:\n",
    "    logger.info('[Write Grouped Features to SageMaker Feature Store]')\n",
    "    success, fail = 0, 0\n",
    "    for record in records:\n",
    "        event_time_feature = {\n",
    "                'FeatureName': 'datetime',\n",
    "                'ValueAsString': str(int(round(time.time())))\n",
    "            }\n",
    "        record.append(event_time_feature)\n",
    "        response = feature_store_client.put_record(FeatureGroupName='cc-train-chime-fg', Record=record)\n",
    "        if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "    logger.info('Success = {}'.format(success))\n",
    "    logger.info('Fail = {}'.format(fail))\n",
    "    assert success == TOTAL_UNIQUE_USERS\n",
    "    assert fail == 0\n",
    "    \n",
    "def run_spark_job():\n",
    "    spark = SparkSession.builder.appName('PySparkJob').getOrCreate()\n",
    "    args = parse_args()\n",
    "    schema = define_schema()\n",
    "    aggregated_features = aggregate_features(args, schema, spark)\n",
    "    \n",
    "#     records = transform_row_train(aggregated_features)\n",
    "#     write_to_train_feature_store(records)\n",
    "#     write_to_s3(args, aggregated_features)\n",
    "#     sliced_df = group_by_card_number(aggregated_features)\n",
    "#     records = transform_row(sliced_df)\n",
    "#     write_to_feature_store(records)\n",
    "\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    run_spark_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Run SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "spark_processor = PySparkProcessor(base_job_name='chime-fs-demo-train', \n",
    "                                   framework_version='2.4', # spark version\n",
    "                                   role=sagemaker_role, \n",
    "                                   instance_count=1, \n",
    "                                   instance_type='ml.m5.12xlarge', \n",
    "                                   env={'AWS_DEFAULT_REGION': boto3.Session().region_name},\n",
    "                                   max_runtime_in_seconds=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating processing-job with name chime-fs-demo-train-2023-04-03-01-07-19-973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  chime-fs-demo-train-2023-04-03-01-07-19-973\n",
      "Inputs:  [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-461312420708/chime-fs-demo-train-2023-04-03-01-07-19-973/input/code/batch_aggregation_train.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://chime-fs-demo/logs', 'LocalPath': '/opt/ml/processing/spark-events/', 'S3UploadMode': 'Continuous'}}]\n",
      ".............................................................................................*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Processing job chime-fs-demo-train-2023-04-03-01-07-19-973: Failed. Reason: AlgorithmError: See job logs for more information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/spark/processing.py:855\u001b[0m, in \u001b[0;36mPySparkProcessor.run\u001b[0;34m(self, submit_app, submit_py_files, submit_jars, submit_files, inputs, outputs, arguments, wait, logs, job_name, experiment_config, configuration, spark_event_logs_s3_uri, kms_key)\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmit_app is required\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    845\u001b[0m extended_inputs, extended_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_processing_args(\n\u001b[1;32m    846\u001b[0m     inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    847\u001b[0m     outputs\u001b[38;5;241m=\u001b[39moutputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    852\u001b[0m     spark_event_logs_s3_uri\u001b[38;5;241m=\u001b[39mspark_event_logs_s3_uri,\n\u001b[1;32m    853\u001b[0m )\n\u001b[0;32m--> 855\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubmit_app\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubmit_app\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43marguments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_current_job_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/spark/processing.py:244\u001b[0m, in \u001b[0;36m_SparkProcessorBase.run\u001b[0;34m(self, submit_app, inputs, outputs, arguments, wait, logs, job_name, experiment_config, kms_key)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs a processing job.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \n\u001b[1;32m    221\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m        user code file (default: None).\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_job_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_current_job_name(job_name\u001b[38;5;241m=\u001b[39mjob_name)\n\u001b[0;32m--> 244\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubmit_app\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43marguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/processing.py:530\u001b[0m, in \u001b[0;36mScriptProcessor.run\u001b[0;34m(self, code, inputs, outputs, arguments, wait, logs, job_name, experiment_config, kms_key)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_job)\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlatest_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/processing.py:925\u001b[0m, in \u001b[0;36mProcessingJob.wait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session\u001b[38;5;241m.\u001b[39mlogs_for_processing_job(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_name, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_processing_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:3159\u001b[0m, in \u001b[0;36mSession.wait_for_processing_job\u001b[0;34m(self, job, poll)\u001b[0m\n\u001b[1;32m   3146\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wait for an Amazon SageMaker Processing job to complete.\u001b[39;00m\n\u001b[1;32m   3147\u001b[0m \n\u001b[1;32m   3148\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3156\u001b[0m \u001b[38;5;124;03m    exceptions.UnexpectedStatusException: If the compilation job fails.\u001b[39;00m\n\u001b[1;32m   3157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3158\u001b[0m desc \u001b[38;5;241m=\u001b[39m _wait_until(\u001b[38;5;28;01mlambda\u001b[39;00m: _processing_job_status(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client, job), poll)\n\u001b[0;32m-> 3159\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_job_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProcessingJobStatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m desc\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/session.py:3288\u001b[0m, in \u001b[0;36mSession._check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3286\u001b[0m reason \u001b[38;5;241m=\u001b[39m desc\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailureReason\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(No reason provided)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3287\u001b[0m job_type \u001b[38;5;241m=\u001b[39m status_key_name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJobStatus\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m job\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3288\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   3289\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError for \u001b[39m\u001b[38;5;132;01m{job_type}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{job_name}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{status}\u001b[39;00m\u001b[38;5;124m. Reason: \u001b[39m\u001b[38;5;132;01m{reason}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   3290\u001b[0m         job_type\u001b[38;5;241m=\u001b[39mjob_type, job_name\u001b[38;5;241m=\u001b[39mjob, status\u001b[38;5;241m=\u001b[39mstatus, reason\u001b[38;5;241m=\u001b[39mreason\n\u001b[1;32m   3291\u001b[0m     ),\n\u001b[1;32m   3292\u001b[0m     allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStopped\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   3293\u001b[0m     actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   3294\u001b[0m )\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Processing job chime-fs-demo-train-2023-04-03-01-07-19-973: Failed. Reason: AlgorithmError: See job logs for more information"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark_processor.run(submit_app='batch_aggregation_train.py', \n",
    "                    arguments=['--s3_input_bucket', BUCKET, \n",
    "                               '--s3_input_key_prefix', INPUT_KEY_PREFIX, \n",
    "                               '--s3_output_bucket', BUCKET, \n",
    "                               '--s3_output_key_prefix', OUTPUT_KEY_PREFIX],\n",
    "                    spark_event_logs_s3_uri='s3://{}/logs'.format(BUCKET),\n",
    "                    logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Explore Aggregated Features \n",
    "<p> The SageMaker Processing Job above creates the aggregated features alongside the raw features and writes it to S3. \n",
    "Let us verify this output using the code below and prep it to be used in the next step for model training.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Copy results csv from S3 to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘./data/aggregated/part*.csv’: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://chime-fs-demo/aggregated/part-00000-1bc78c44-62fe-4425-8ac9-9b9a367da961-c000.csv to data/aggregated/part-00000-1bc78c44-62fe-4425-8ac9-9b9a367da961-c000.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://{BUCKET}/{OUTPUT_KEY_PREFIX}/ {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/ --recursive --exclude '_SUCCESS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "!mv {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part*.csv {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>amount</th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>num_trans_last_10m</th>\n",
       "      <th>avg_amt_last_10m</th>\n",
       "      <th>num_trans_last_1w</th>\n",
       "      <th>avg_amt_last_1w</th>\n",
       "      <th>amt_ratio1</th>\n",
       "      <th>amt_ratio2</th>\n",
       "      <th>count_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d621c8d794262ad5e8ad804cb4517395</td>\n",
       "      <td>2020-01-01T20:52:20.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>8911.09</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8911.09</td>\n",
       "      <td>1</td>\n",
       "      <td>8911.090000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>daa28b6f0e729f48563ee7ea945f910c</td>\n",
       "      <td>2020-01-02T00:51:07.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>65.15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>65.15</td>\n",
       "      <td>2</td>\n",
       "      <td>4488.120000</td>\n",
       "      <td>0.014516</td>\n",
       "      <td>0.014516</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c4f86514d36cf92be555c122f2faae57</td>\n",
       "      <td>2020-01-02T01:08:17.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>4865.93</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4865.93</td>\n",
       "      <td>3</td>\n",
       "      <td>4614.056667</td>\n",
       "      <td>1.054588</td>\n",
       "      <td>1.054588</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0758642b10c11900f42b880c9cb1527b</td>\n",
       "      <td>2020-01-02T09:50:42.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>11.92</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11.92</td>\n",
       "      <td>4</td>\n",
       "      <td>3463.522500</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79dbe8dbe6d193de62cc4adad8fc6d4f</td>\n",
       "      <td>2020-01-03T00:05:41.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>626.46</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>626.46</td>\n",
       "      <td>5</td>\n",
       "      <td>2896.110000</td>\n",
       "      <td>0.216311</td>\n",
       "      <td>0.216311</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                tid                  datetime  \\\n",
       "0  d621c8d794262ad5e8ad804cb4517395  2020-01-01T20:52:20.000Z   \n",
       "1  daa28b6f0e729f48563ee7ea945f910c  2020-01-02T00:51:07.000Z   \n",
       "2  c4f86514d36cf92be555c122f2faae57  2020-01-02T01:08:17.000Z   \n",
       "3  0758642b10c11900f42b880c9cb1527b  2020-01-02T09:50:42.000Z   \n",
       "4  79dbe8dbe6d193de62cc4adad8fc6d4f  2020-01-03T00:05:41.000Z   \n",
       "\n",
       "             cc_num   amount  fraud_label  num_trans_last_10m  \\\n",
       "0  4006080197832643  8911.09            0                   1   \n",
       "1  4006080197832643    65.15            0                   1   \n",
       "2  4006080197832643  4865.93            0                   1   \n",
       "3  4006080197832643    11.92            0                   1   \n",
       "4  4006080197832643   626.46            0                   1   \n",
       "\n",
       "   avg_amt_last_10m  num_trans_last_1w  avg_amt_last_1w  amt_ratio1  \\\n",
       "0           8911.09                  1      8911.090000    1.000000   \n",
       "1             65.15                  2      4488.120000    0.014516   \n",
       "2           4865.93                  3      4614.056667    1.054588   \n",
       "3             11.92                  4      3463.522500    0.003442   \n",
       "4            626.46                  5      2896.110000    0.216311   \n",
       "\n",
       "   amt_ratio2  count_ratio  \n",
       "0    1.000000     1.000000  \n",
       "1    0.014516     0.500000  \n",
       "2    1.054588     0.333333  \n",
       "3    0.003442     0.250000  \n",
       "4    0.216311     0.200000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_features = pd.read_csv(f'{LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part.csv')\n",
    "agg_features.dropna(inplace=True)\n",
    "agg_features['cc_num'] = agg_features['cc_num'].astype(np.int64)\n",
    "agg_features['fraud_label'] = agg_features['fraud_label'].astype(np.int64)\n",
    "agg_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "agg_features.to_csv(f'{LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/processing_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Remove the intermediate `part.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "!rm {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LOCAL_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m fail \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mLOCAL_DIR\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_KEY_PREFIX\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/processing_output.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     45\u001b[0m df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# records = transform_row(df)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# write_to_feature_store(records)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LOCAL_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "def transform_row(df) -> list:\n",
    "    logger.info('[Transform Spark DataFrame Row to SageMaker Feature Store Record]')\n",
    "    records = []\n",
    "    for index, row in df.iterrows():\n",
    "        record = []\n",
    "        tid,cc_num, amount, fraud_label, num_trans_last_10m, avg_amt_last_10m, num_trans_last_1w, avg_amt_last_1w,datetime, amt_ratio1,amt_ratio2,count_ratio = row\n",
    "        if tid:\n",
    "            record.append({'ValueAsString': str(tid), 'FeatureName': 'tid'})\n",
    "            record.append({'ValueAsString': str(cc_num), 'FeatureName': 'cc_num'})\n",
    "            record.append({'ValueAsString': str(round(amount, 2)), 'FeatureName': 'amount'})\n",
    "            record.append({'ValueAsString': str(fraud_label), 'FeatureName': 'fraud_label'})\n",
    "            record.append({'ValueAsString': str(num_trans_last_1w), 'FeatureName': 'num_trans_last_1w'})\n",
    "            record.append({'ValueAsString': str(round(avg_amt_last_1w, 2)), 'FeatureName': 'avg_amt_last_1w'})\n",
    "            record.append({'ValueAsString': str(datetime), 'FeatureName': 'datetime'})\n",
    "            record.append({'ValueAsString': str(num_trans_last_10m), 'FeatureName': 'num_trans_last_10m'})\n",
    "            record.append({'ValueAsString': str(round(avg_amt_last_10m, 2)), 'FeatureName': 'avg_amt_last_10m'})\n",
    "            record.append({'ValueAsString': str(round(amt_ratio1, 2)), 'FeatureName': 'amt_ratio1'})\n",
    "            record.append({'ValueAsString': str(round(amt_ratio2, 2)), 'FeatureName': 'amt_ratio2'})\n",
    "            record.append({'ValueAsString': str(round(count_ratio, 2)), 'FeatureName': 'count_ratio'})\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "def write_to_feature_store(records: list) -> None:\n",
    "    logger.info('[Write Grouped Features to SageMaker Feature Store]')\n",
    "    success, fail = 0, 0\n",
    "    for record in records:\n",
    "        event_time_feature = {\n",
    "                'FeatureName': 'datetime',\n",
    "                'ValueAsString': str(int(round(time.time())))\n",
    "            }\n",
    "        record.append(event_time_feature)\n",
    "        response = feature_store_client.put_record(FeatureGroupName='cc-train-chime-fg', Record=record)\n",
    "        if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "    logger.info('Success = {}'.format(success))\n",
    "    logger.info('Fail = {}'.format(fail))\n",
    "    assert success == TOTAL_UNIQUE_USERS\n",
    "    assert fail == 0\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f'{LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/processing_output.csv')\n",
    "df.iloc[1]\n",
    "# records = transform_row(df)\n",
    "# write_to_feature_store(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Validate Feature Group for Records\n",
    "Let's randomly pick N credit card numbers from the `processing_output.csv` and verify if records exist in the feature group `cc-agg-batch-fg` for these card numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "N = 3 # number of random records to validate\n",
    "FEATURE_GROUP = 'cc-agg-batch-fg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>amount</th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>num_trans_last_10m</th>\n",
       "      <th>avg_amt_last_10m</th>\n",
       "      <th>num_trans_last_1w</th>\n",
       "      <th>avg_amt_last_1w</th>\n",
       "      <th>amt_ratio1</th>\n",
       "      <th>amt_ratio2</th>\n",
       "      <th>count_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afc4fe351c04c5b81de2437524fe7771</td>\n",
       "      <td>2020-01-01T12:54:54.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>657.47</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>657.47</td>\n",
       "      <td>1</td>\n",
       "      <td>657.470000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7fc373846810b3d5b70edcad77b5c944</td>\n",
       "      <td>2020-01-01T22:52:37.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>11.17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11.17</td>\n",
       "      <td>2</td>\n",
       "      <td>334.320000</td>\n",
       "      <td>0.033411</td>\n",
       "      <td>0.033411</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eb37b0ec5dae2e6e49a9949a47b0ab4a</td>\n",
       "      <td>2020-01-01T23:47:32.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>69.62</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>69.62</td>\n",
       "      <td>3</td>\n",
       "      <td>246.086667</td>\n",
       "      <td>0.282908</td>\n",
       "      <td>0.282908</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6d24d9b143ff8035f802f131ebfe993f</td>\n",
       "      <td>2020-01-02T03:45:45.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>1187.41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1187.41</td>\n",
       "      <td>4</td>\n",
       "      <td>481.417500</td>\n",
       "      <td>2.466487</td>\n",
       "      <td>2.466487</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ea52ff23a868b6a3ed2f45afc1c7f406</td>\n",
       "      <td>2020-01-02T05:06:46.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>981.31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>981.31</td>\n",
       "      <td>5</td>\n",
       "      <td>581.396000</td>\n",
       "      <td>1.687851</td>\n",
       "      <td>1.687851</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                tid                  datetime  \\\n",
       "0  afc4fe351c04c5b81de2437524fe7771  2020-01-01T12:54:54.000Z   \n",
       "1  7fc373846810b3d5b70edcad77b5c944  2020-01-01T22:52:37.000Z   \n",
       "2  eb37b0ec5dae2e6e49a9949a47b0ab4a  2020-01-01T23:47:32.000Z   \n",
       "3  6d24d9b143ff8035f802f131ebfe993f  2020-01-02T03:45:45.000Z   \n",
       "4  ea52ff23a868b6a3ed2f45afc1c7f406  2020-01-02T05:06:46.000Z   \n",
       "\n",
       "             cc_num   amount  fraud_label  num_trans_last_10m  \\\n",
       "0  4006080197832643   657.47            0                   1   \n",
       "1  4006080197832643    11.17            0                   1   \n",
       "2  4006080197832643    69.62            0                   1   \n",
       "3  4006080197832643  1187.41            0                   1   \n",
       "4  4006080197832643   981.31            0                   1   \n",
       "\n",
       "   avg_amt_last_10m  num_trans_last_1w  avg_amt_last_1w  amt_ratio1  \\\n",
       "0            657.47                  1       657.470000    1.000000   \n",
       "1             11.17                  2       334.320000    0.033411   \n",
       "2             69.62                  3       246.086667    0.282908   \n",
       "3           1187.41                  4       481.417500    2.466487   \n",
       "4            981.31                  5       581.396000    1.687851   \n",
       "\n",
       "   amt_ratio2  count_ratio  \n",
       "0    1.000000     1.000000  \n",
       "1    0.033411     0.500000  \n",
       "2    0.282908     0.333333  \n",
       "3    2.466487     0.250000  \n",
       "4    1.687851     0.200000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_out_df = pd.read_csv(f'{LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/processing_output.csv')\n",
    "processing_out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4987034976018745, 4647964443566763, 4445477846392608]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_nums = random.sample(processing_out_df['cc_num'].tolist(), N)\n",
    "cc_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Using SageMaker Feature Store run-time client, we can verify if records exist in the feature group for the picked `cc_nums` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "feature_store_client = boto3.Session().client(service_name='sagemaker-featurestore-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'FeatureName': 'cc_num', 'ValueAsString': '4987034976018745'}, {'FeatureName': 'num_trans_last_1w', 'ValueAsString': '30'}, {'FeatureName': 'avg_amt_last_1w', 'ValueAsString': '363.93'}, {'FeatureName': 'trans_time', 'ValueAsString': '1679691906'}]\n",
      "[{'FeatureName': 'cc_num', 'ValueAsString': '4647964443566763'}, {'FeatureName': 'num_trans_last_1w', 'ValueAsString': '24'}, {'FeatureName': 'avg_amt_last_1w', 'ValueAsString': '437.18'}, {'FeatureName': 'trans_time', 'ValueAsString': '1679691946'}]\n",
      "[{'FeatureName': 'cc_num', 'ValueAsString': '4445477846392608'}, {'FeatureName': 'num_trans_last_1w', 'ValueAsString': '23'}, {'FeatureName': 'avg_amt_last_1w', 'ValueAsString': '367.41'}, {'FeatureName': 'trans_time', 'ValueAsString': '1679691975'}]\n"
     ]
    }
   ],
   "source": [
    "success, fail = 0, 0\n",
    "for cc_num in cc_nums:\n",
    "    response = feature_store_client.get_record(FeatureGroupName=FEATURE_GROUP, \n",
    "                                               RecordIdentifierValueAsString=str(cc_num))\n",
    "    if response['ResponseMetadata']['HTTPStatusCode'] == 200 and 'Record' in response.keys():\n",
    "        success += 1\n",
    "        print(response['Record'])\n",
    "    else:\n",
    "        print(response)\n",
    "        fail += 1\n",
    "assert success == N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
