{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Batch Ingestion\n",
    "**This notebook aggregates raw features into new derived features that is used for Fraud Detection model training/inference.**\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Create PySpark Processing Script](#Create-PySpark-Processing-Script)\n",
    "1. [Run SageMaker Processing Job](#Run-SageMaker-Processing-Job)\n",
    "1. [Explore Aggregated Features](#Explore-Aggregated-Features)\n",
    "1. [Validate Feature Group for Records](#Validate-Feature-Group-for-Records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Background\n",
    "\n",
    "- This notebook takes raw credit card transactions data (csv) generated by \n",
    "[notebook 0](./0_prepare_transactions_dataset.ipynb) and aggregates the raw features to create new features (ratios) via <b>SageMaker Processing</b> PySpark Job. These aggregated features alongside the raw original features will be leveraged in the training phase of a Credit Card Fraud Detection model in the next step (see notebook [notebook 3](./3_train_and_deploy_model.ipynb)).\n",
    "\n",
    "- As part of the Spark job, we also select the latest weekly aggregated features - `num_trans_last_1w` and `avg_amt_last_1w` grouped by `cc_num` (credit card number) and populate these features into the <b>SageMaker Online Feature Store</b> as a feature group. This feature group (`cc-agg-batch-fg`) was created in notebook [notebook 1](./1_setup.ipynb).\n",
    "\n",
    "- [Amazon SageMaker Processing](https://aws.amazon.com/about-aws/whats-new/2020/09/amazon-sagemaker-processing-now-supports-built-in-spark-containers-for-big-data-processing/) lets customers run analytics jobs for data engineering and model evaluation on Amazon SageMaker easily and at scale. It provides a fully managed Spark environment for data processing or feature engineering workloads.\n",
    "\n",
    "<img src=\"./images/batch_ingestion.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import logging\n",
    "import random\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker version: 2.145.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Using SageMaker version: {sagemaker.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Setup Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Batch Aggregation using SageMaker PySpark Processing Job]\n"
     ]
    }
   ],
   "source": [
    "logger.info('[Batch Aggregation using SageMaker PySpark Processing Job]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "BUCKET = 'sm-fs-demo'\n",
    "INPUT_KEY_PREFIX = 'raw'\n",
    "OUTPUT_KEY_PREFIX = 'aggregated'\n",
    "LOCAL_DIR = './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Create PySpark Script\n",
    "This PySpark script does the following:\n",
    "\n",
    "1. Aggregates raw features to derive new features (ratios).\n",
    "2. Saves the aggregated features alongside the original raw features into a CSV file and writes it to S3 - will be used in the next step for model training.\n",
    "3. Groups the aggregated features by credit card number and picks selected aggregated features to write to SageMaker Feature Store (Online). <br>\n",
    "<b>Note: </b> The feature group was created in the previous notebook (`1_setup.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing batch_aggregation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile batch_aggregation.py\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, TimestampType, LongType\n",
    "from pyspark.sql.functions import desc, dense_rank\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from  argparse import Namespace, ArgumentParser\n",
    "from pyspark.sql.window import Window\n",
    "import argparse\n",
    "import logging\n",
    "import boto3\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "TOTAL_UNIQUE_USERS = 10000\n",
    "FEATURE_GROUP = 'cc-agg-batch-fg'\n",
    "\n",
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "feature_store_client = boto3.client(service_name='sagemaker-featurestore-runtime')\n",
    "\n",
    "\n",
    "def parse_args() -> Namespace:\n",
    "    parser = ArgumentParser(description='Spark Job Input and Output Args')\n",
    "    parser.add_argument('--s3_input_bucket', type=str, help='S3 Input Bucket')\n",
    "    parser.add_argument('--s3_input_key_prefix', type=str, help='S3 Input Key Prefix')\n",
    "    parser.add_argument('--s3_output_bucket', type=str, help='S3 Output Bucket')\n",
    "    parser.add_argument('--s3_output_key_prefix', type=str, help='S3 Output Key Prefix')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "    \n",
    "\n",
    "def define_schema() -> StructType:\n",
    "    schema = StructType([StructField('tid', StringType(), True),\n",
    "                         StructField('datetime', TimestampType(), True),\n",
    "                         StructField('cc_num', LongType(), True),\n",
    "                         StructField('amount', DoubleType(), True),\n",
    "                         StructField('fraud_label', StringType(), True)])\n",
    "    return schema\n",
    "\n",
    "\n",
    "def aggregate_features(args: Namespace, schema: StructType, spark: SparkSession) -> DataFrame:\n",
    "    logger.info('[Read Raw Transactions Data as Spark DataFrame]')\n",
    "    transactions_df = spark.read.csv(f's3a://{os.path.join(args.s3_input_bucket, args.s3_input_key_prefix)}', \\\n",
    "                                     header=False, \\\n",
    "                                     schema=schema)\n",
    "    logger.info('[Aggregate Transactions to Derive New Features using Spark SQL]')\n",
    "    query = \"\"\"\n",
    "    SELECT *, \\\n",
    "           avg_amt_last_10m/avg_amt_last_1w AS amt_ratio1, \\\n",
    "           amount/avg_amt_last_1w AS amt_ratio2, \\\n",
    "           num_trans_last_10m/num_trans_last_1w AS count_ratio \\\n",
    "    FROM \\\n",
    "        ( \\\n",
    "        SELECT *, \\\n",
    "               COUNT(*) OVER w1 as num_trans_last_10m, \\\n",
    "               AVG(amount) OVER w1 as avg_amt_last_10m, \\\n",
    "               COUNT(*) OVER w2 as num_trans_last_1w, \\\n",
    "               AVG(amount) OVER w2 as avg_amt_last_1w \\\n",
    "        FROM transactions_df \\\n",
    "        WINDOW \\\n",
    "               w1 AS (PARTITION BY cc_num order by cast(datetime AS timestamp) RANGE INTERVAL 10 MINUTE PRECEDING), \\\n",
    "               w2 AS (PARTITION BY cc_num order by cast(datetime AS timestamp) RANGE INTERVAL 1 WEEK PRECEDING) \\\n",
    "        ) \n",
    "    \"\"\"\n",
    "    transactions_df.registerTempTable('transactions_df')\n",
    "    aggregated_features = spark.sql(query)\n",
    "    return aggregated_features\n",
    "\n",
    "\n",
    "def write_to_s3(args: Namespace, aggregated_features: DataFrame) -> None:\n",
    "    logger.info('[Write Aggregated Features to S3]')\n",
    "    aggregated_features.coalesce(1) \\\n",
    "                       .write.format('com.databricks.spark.csv') \\\n",
    "                       .option('header', True) \\\n",
    "                       .mode('overwrite') \\\n",
    "                       .option('sep', ',') \\\n",
    "                       .save('s3a://' + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix))\n",
    "    \n",
    "def group_by_card_number(aggregated_features: DataFrame) -> DataFrame: \n",
    "    logger.info('[Group Aggregated Features by Card Number]')\n",
    "    window = Window.partitionBy('cc_num').orderBy(desc('datetime'))\n",
    "    sorted_df = aggregated_features.withColumn('rank', dense_rank().over(window))\n",
    "    grouped_df = sorted_df.filter(sorted_df.rank == 1).drop(sorted_df.rank)\n",
    "    sliced_df = grouped_df.select('cc_num', 'num_trans_last_1w', 'avg_amt_last_1w')\n",
    "    return sliced_df\n",
    "\n",
    "\n",
    "def transform_row(sliced_df: DataFrame) -> list:\n",
    "    logger.info('[Transform Spark DataFrame Row to SageMaker Feature Store Record]')\n",
    "    records = []\n",
    "    for row in sliced_df.rdd.collect():\n",
    "        record = []\n",
    "        cc_num, num_trans_last_1w, avg_amt_last_1w = row\n",
    "        if cc_num:\n",
    "            record.append({'ValueAsString': str(cc_num), 'FeatureName': 'cc_num'})\n",
    "            record.append({'ValueAsString': str(num_trans_last_1w), 'FeatureName': 'num_trans_last_1w'})\n",
    "            record.append({'ValueAsString': str(round(avg_amt_last_1w, 2)), 'FeatureName': 'avg_amt_last_1w'})\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "\n",
    "def write_to_feature_store(records: list) -> None:\n",
    "    logger.info('[Write Grouped Features to SageMaker Online Feature Store]')\n",
    "    success, fail = 0, 0\n",
    "    for record in records:\n",
    "        event_time_feature = {\n",
    "                'FeatureName': 'trans_time',\n",
    "                'ValueAsString': str(int(round(time.time())))\n",
    "            }\n",
    "        record.append(event_time_feature)\n",
    "        response = feature_store_client.put_record(FeatureGroupName=FEATURE_GROUP, Record=record)\n",
    "        if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "    logger.info('Success = {}'.format(success))\n",
    "    logger.info('Fail = {}'.format(fail))\n",
    "    assert success == TOTAL_UNIQUE_USERS\n",
    "    assert fail == 0\n",
    "\n",
    "\n",
    "def run_spark_job():\n",
    "    spark = SparkSession.builder.appName('PySparkJob').getOrCreate()\n",
    "    args = parse_args()\n",
    "    schema = define_schema()\n",
    "    aggregated_features = aggregate_features(args, schema, spark)\n",
    "    write_to_s3(args, aggregated_features)\n",
    "    sliced_df = group_by_card_number(aggregated_features)\n",
    "    records = transform_row(sliced_df)\n",
    "    write_to_feature_store(records)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    run_spark_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Run SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "spark_processor = PySparkProcessor(base_job_name='sm-fs-demo', \n",
    "                                   framework_version='2.4', # spark version\n",
    "                                   role=sagemaker_role, \n",
    "                                   instance_count=1, \n",
    "                                   instance_type='ml.m5.2xlarge', \n",
    "                                   env={'AWS_DEFAULT_REGION': boto3.Session().region_name},\n",
    "                                   max_runtime_in_seconds=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating processing-job with name sm-fs-demo-2023-04-09-18-35-08-914\n",
      "INFO:sagemaker:Creating processing-job with name sm-fs-demo-2023-04-09-18-35-08-914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................!CPU times: user 701 ms, sys: 25.4 ms, total: 727 ms\n",
      "Wall time: 8min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark_processor.run(submit_app='batch_aggregation.py', \n",
    "                    arguments=['--s3_input_bucket', BUCKET, \n",
    "                               '--s3_input_key_prefix', INPUT_KEY_PREFIX, \n",
    "                               '--s3_output_bucket', BUCKET, \n",
    "                               '--s3_output_key_prefix', OUTPUT_KEY_PREFIX],\n",
    "                    spark_event_logs_s3_uri='s3://{}/logs'.format(BUCKET),\n",
    "                    logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Explore Aggregated Features \n",
    "<p> The SageMaker Processing Job above creates the aggregated features alongside the raw features and writes it to S3. \n",
    "Let us verify this output using the code below and prep it to be used in the next step for model training.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Copy results csv from S3 to local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove ‘./data/aggregated/part*.csv’: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!rm {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sm-fs-demo/aggregated/part-00000-4af03980-7edc-414a-bf3f-40645cf9d130-c000.csv to data/aggregated/part-00000-4af03980-7edc-414a-bf3f-40645cf9d130-c000.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://{BUCKET}/{OUTPUT_KEY_PREFIX}/ {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/ --recursive --exclude '_SUCCESS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "!mv {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part*.csv {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>amount</th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>num_trans_last_10m</th>\n",
       "      <th>avg_amt_last_10m</th>\n",
       "      <th>num_trans_last_1w</th>\n",
       "      <th>avg_amt_last_1w</th>\n",
       "      <th>amt_ratio1</th>\n",
       "      <th>amt_ratio2</th>\n",
       "      <th>count_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9865906a3fc8ffb36edd7413302fd50d</td>\n",
       "      <td>2020-01-01T08:03:37.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>89.69</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>89.69</td>\n",
       "      <td>1</td>\n",
       "      <td>89.690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b18b52528c812800f93c815139480f1f</td>\n",
       "      <td>2020-01-01T11:23:16.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>57.98</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>57.98</td>\n",
       "      <td>2</td>\n",
       "      <td>73.835</td>\n",
       "      <td>0.785264</td>\n",
       "      <td>0.785264</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34b9c71ea65c2a9003dd67e65e4507ec</td>\n",
       "      <td>2020-01-02T03:45:27.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>195.62</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>195.62</td>\n",
       "      <td>3</td>\n",
       "      <td>114.430</td>\n",
       "      <td>1.709517</td>\n",
       "      <td>1.709517</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>610a9e76bd6a3fb56bed7501b3ec6c0e</td>\n",
       "      <td>2020-01-02T07:14:02.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>653.63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>653.63</td>\n",
       "      <td>4</td>\n",
       "      <td>249.230</td>\n",
       "      <td>2.622598</td>\n",
       "      <td>2.622598</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c24caf694d7b5375b4a72224c5351949</td>\n",
       "      <td>2020-01-02T16:43:26.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>20.18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.18</td>\n",
       "      <td>5</td>\n",
       "      <td>203.420</td>\n",
       "      <td>0.099204</td>\n",
       "      <td>0.099204</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                tid                  datetime  \\\n",
       "0  9865906a3fc8ffb36edd7413302fd50d  2020-01-01T08:03:37.000Z   \n",
       "1  b18b52528c812800f93c815139480f1f  2020-01-01T11:23:16.000Z   \n",
       "2  34b9c71ea65c2a9003dd67e65e4507ec  2020-01-02T03:45:27.000Z   \n",
       "3  610a9e76bd6a3fb56bed7501b3ec6c0e  2020-01-02T07:14:02.000Z   \n",
       "4  c24caf694d7b5375b4a72224c5351949  2020-01-02T16:43:26.000Z   \n",
       "\n",
       "             cc_num  amount  fraud_label  num_trans_last_10m  \\\n",
       "0  4006080197832643   89.69            0                   1   \n",
       "1  4006080197832643   57.98            0                   1   \n",
       "2  4006080197832643  195.62            0                   1   \n",
       "3  4006080197832643  653.63            0                   1   \n",
       "4  4006080197832643   20.18            0                   1   \n",
       "\n",
       "   avg_amt_last_10m  num_trans_last_1w  avg_amt_last_1w  amt_ratio1  \\\n",
       "0             89.69                  1           89.690    1.000000   \n",
       "1             57.98                  2           73.835    0.785264   \n",
       "2            195.62                  3          114.430    1.709517   \n",
       "3            653.63                  4          249.230    2.622598   \n",
       "4             20.18                  5          203.420    0.099204   \n",
       "\n",
       "   amt_ratio2  count_ratio  \n",
       "0    1.000000     1.000000  \n",
       "1    0.785264     0.500000  \n",
       "2    1.709517     0.333333  \n",
       "3    2.622598     0.250000  \n",
       "4    0.099204     0.200000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_features = pd.read_csv(f'{LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part.csv')\n",
    "agg_features.dropna(inplace=True)\n",
    "agg_features['cc_num'] = agg_features['cc_num'].astype(np.int64)\n",
    "agg_features['fraud_label'] = agg_features['fraud_label'].astype(np.int64)\n",
    "agg_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "agg_features.to_csv(f'{LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/processing_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Remove the intermediate `part.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "!rm {LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/part.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def transform_row(df) -> list:\n",
    "#     logger.info('[Transform Spark DataFrame Row to SageMaker Feature Store Record]')\n",
    "#     records = []\n",
    "#     for index, row in df.iterrows():\n",
    "#         record = []\n",
    "#         tid,cc_num, amount, fraud_label, num_trans_last_10m, avg_amt_last_10m, num_trans_last_1w, avg_amt_last_1w,datetime, amt_ratio1,amt_ratio2,count_ratio = row\n",
    "#         if tid:\n",
    "#             record.append({'ValueAsString': str(tid), 'FeatureName': 'tid'})\n",
    "#             record.append({'ValueAsString': str(cc_num), 'FeatureName': 'cc_num'})\n",
    "#             record.append({'ValueAsString': str(round(amount, 2)), 'FeatureName': 'amount'})\n",
    "#             record.append({'ValueAsString': str(fraud_label), 'FeatureName': 'fraud_label'})\n",
    "#             record.append({'ValueAsString': str(num_trans_last_1w), 'FeatureName': 'num_trans_last_1w'})\n",
    "#             record.append({'ValueAsString': str(round(avg_amt_last_1w, 2)), 'FeatureName': 'avg_amt_last_1w'})\n",
    "#             record.append({'ValueAsString': str(datetime), 'FeatureName': 'datetime'})\n",
    "#             record.append({'ValueAsString': str(num_trans_last_10m), 'FeatureName': 'num_trans_last_10m'})\n",
    "#             record.append({'ValueAsString': str(round(avg_amt_last_10m, 2)), 'FeatureName': 'avg_amt_last_10m'})\n",
    "#             record.append({'ValueAsString': str(round(amt_ratio1, 2)), 'FeatureName': 'amt_ratio1'})\n",
    "#             record.append({'ValueAsString': str(round(amt_ratio2, 2)), 'FeatureName': 'amt_ratio2'})\n",
    "#             record.append({'ValueAsString': str(round(count_ratio, 2)), 'FeatureName': 'count_ratio'})\n",
    "#             records.append(record)\n",
    "#     return records\n",
    "\n",
    "# def write_to_feature_store(records: list) -> None:\n",
    "#     logger.info('[Write Grouped Features to SageMaker Feature Store]')\n",
    "#     success, fail = 0, 0\n",
    "#     for record in records:\n",
    "#         event_time_feature = {\n",
    "#                 'FeatureName': 'datetime',\n",
    "#                 'ValueAsString': str(int(round(time.time())))\n",
    "#             }\n",
    "#         record.append(event_time_feature)\n",
    "#         response = feature_store_client.put_record(FeatureGroupName='cc-train-chime-fg', Record=record)\n",
    "#         if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "#             success += 1\n",
    "#         else:\n",
    "#             fail += 1\n",
    "#     logger.info('Success = {}'.format(success))\n",
    "#     logger.info('Fail = {}'.format(fail))\n",
    "#     assert success == TOTAL_UNIQUE_USERS\n",
    "#     assert fail == 0\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv(f'{LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/processing_output.csv')\n",
    "# df.iloc[1]\n",
    "\n",
    "# records = transform_row(df)\n",
    "# write_to_feature_store(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Validate Feature Group for Records\n",
    "Let's randomly pick N credit card numbers from the `processing_output.csv` and verify if records exist in the feature group `cc-agg-batch-fg` for these card numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "N = 3 # number of random records to validate\n",
    "FEATURE_GROUP = 'cc-agg-batch-fg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cc_num</th>\n",
       "      <th>amount</th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>num_trans_last_10m</th>\n",
       "      <th>avg_amt_last_10m</th>\n",
       "      <th>num_trans_last_1w</th>\n",
       "      <th>avg_amt_last_1w</th>\n",
       "      <th>amt_ratio1</th>\n",
       "      <th>amt_ratio2</th>\n",
       "      <th>count_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9865906a3fc8ffb36edd7413302fd50d</td>\n",
       "      <td>2020-01-01T08:03:37.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>89.69</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>89.69</td>\n",
       "      <td>1</td>\n",
       "      <td>89.690</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b18b52528c812800f93c815139480f1f</td>\n",
       "      <td>2020-01-01T11:23:16.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>57.98</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>57.98</td>\n",
       "      <td>2</td>\n",
       "      <td>73.835</td>\n",
       "      <td>0.785264</td>\n",
       "      <td>0.785264</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34b9c71ea65c2a9003dd67e65e4507ec</td>\n",
       "      <td>2020-01-02T03:45:27.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>195.62</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>195.62</td>\n",
       "      <td>3</td>\n",
       "      <td>114.430</td>\n",
       "      <td>1.709517</td>\n",
       "      <td>1.709517</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>610a9e76bd6a3fb56bed7501b3ec6c0e</td>\n",
       "      <td>2020-01-02T07:14:02.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>653.63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>653.63</td>\n",
       "      <td>4</td>\n",
       "      <td>249.230</td>\n",
       "      <td>2.622598</td>\n",
       "      <td>2.622598</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c24caf694d7b5375b4a72224c5351949</td>\n",
       "      <td>2020-01-02T16:43:26.000Z</td>\n",
       "      <td>4006080197832643</td>\n",
       "      <td>20.18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.18</td>\n",
       "      <td>5</td>\n",
       "      <td>203.420</td>\n",
       "      <td>0.099204</td>\n",
       "      <td>0.099204</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                tid                  datetime  \\\n",
       "0  9865906a3fc8ffb36edd7413302fd50d  2020-01-01T08:03:37.000Z   \n",
       "1  b18b52528c812800f93c815139480f1f  2020-01-01T11:23:16.000Z   \n",
       "2  34b9c71ea65c2a9003dd67e65e4507ec  2020-01-02T03:45:27.000Z   \n",
       "3  610a9e76bd6a3fb56bed7501b3ec6c0e  2020-01-02T07:14:02.000Z   \n",
       "4  c24caf694d7b5375b4a72224c5351949  2020-01-02T16:43:26.000Z   \n",
       "\n",
       "             cc_num  amount  fraud_label  num_trans_last_10m  \\\n",
       "0  4006080197832643   89.69            0                   1   \n",
       "1  4006080197832643   57.98            0                   1   \n",
       "2  4006080197832643  195.62            0                   1   \n",
       "3  4006080197832643  653.63            0                   1   \n",
       "4  4006080197832643   20.18            0                   1   \n",
       "\n",
       "   avg_amt_last_10m  num_trans_last_1w  avg_amt_last_1w  amt_ratio1  \\\n",
       "0             89.69                  1           89.690    1.000000   \n",
       "1             57.98                  2           73.835    0.785264   \n",
       "2            195.62                  3          114.430    1.709517   \n",
       "3            653.63                  4          249.230    2.622598   \n",
       "4             20.18                  5          203.420    0.099204   \n",
       "\n",
       "   amt_ratio2  count_ratio  \n",
       "0    1.000000     1.000000  \n",
       "1    0.785264     0.500000  \n",
       "2    1.709517     0.333333  \n",
       "3    2.622598     0.250000  \n",
       "4    0.099204     0.200000  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_out_df = pd.read_csv(f'{LOCAL_DIR}/{OUTPUT_KEY_PREFIX}/processing_output.csv')\n",
    "processing_out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4667414786921170, 4978884251790033, 4466790039670992]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_nums = random.sample(processing_out_df['cc_num'].tolist(), N)\n",
    "cc_nums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Using SageMaker Feature Store run-time client, we can verify if records exist in the feature group for the picked `cc_nums` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "feature_store_client = boto3.Session().client(service_name='sagemaker-featurestore-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'FeatureName': 'cc_num', 'ValueAsString': '4667414786921170'}, {'FeatureName': 'num_trans_last_1w', 'ValueAsString': '21'}, {'FeatureName': 'avg_amt_last_1w', 'ValueAsString': '763.85'}, {'FeatureName': 'trans_time', 'ValueAsString': '1681065736'}]\n",
      "[{'FeatureName': 'cc_num', 'ValueAsString': '4978884251790033'}, {'FeatureName': 'num_trans_last_1w', 'ValueAsString': '24'}, {'FeatureName': 'avg_amt_last_1w', 'ValueAsString': '763.33'}, {'FeatureName': 'trans_time', 'ValueAsString': '1681065790'}]\n",
      "[{'FeatureName': 'cc_num', 'ValueAsString': '4466790039670992'}, {'FeatureName': 'num_trans_last_1w', 'ValueAsString': '22'}, {'FeatureName': 'avg_amt_last_1w', 'ValueAsString': '1863.17'}, {'FeatureName': 'trans_time', 'ValueAsString': '1681065784'}]\n"
     ]
    }
   ],
   "source": [
    "success, fail = 0, 0\n",
    "for cc_num in cc_nums:\n",
    "    response = feature_store_client.get_record(FeatureGroupName=FEATURE_GROUP, \n",
    "                                               RecordIdentifierValueAsString=str(cc_num))\n",
    "    if response['ResponseMetadata']['HTTPStatusCode'] == 200 and 'Record' in response.keys():\n",
    "        success += 1\n",
    "        print(response['Record'])\n",
    "    else:\n",
    "        print(response)\n",
    "        fail += 1\n",
    "assert success == N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Transaction Feature Group for Training\n",
    "Let's read the `processing_output.csv` file and create and ingest to the new feature group `cc-trans-fg` with all the transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tid                   5400000\n",
       "datetime              5400000\n",
       "cc_num                5400000\n",
       "amount                5400000\n",
       "fraud_label           5400000\n",
       "num_trans_last_10m    5400000\n",
       "avg_amt_last_10m      5400000\n",
       "num_trans_last_1w     5400000\n",
       "avg_amt_last_1w       5400000\n",
       "amt_ratio1            5400000\n",
       "amt_ratio2            5400000\n",
       "count_ratio           5400000\n",
       "datetime1             5400000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing_out_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "\n",
    "\n",
    "feature_store_session = Session(\n",
    "    boto_session=boto_session,\n",
    "    sagemaker_client=sagemaker_client,\n",
    "    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tid</th>\n",
       "      <th>datetime</th>\n",
       "      <th>fraud_label</th>\n",
       "      <th>amount</th>\n",
       "      <th>amt_ratio1</th>\n",
       "      <th>amt_ratio2</th>\n",
       "      <th>count_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9865906a3fc8ffb36edd7413302fd50d</td>\n",
       "      <td>2020-01-01T08:03:37.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>89.69</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b18b52528c812800f93c815139480f1f</td>\n",
       "      <td>2020-01-01T11:23:16.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>57.98</td>\n",
       "      <td>0.785264</td>\n",
       "      <td>0.785264</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34b9c71ea65c2a9003dd67e65e4507ec</td>\n",
       "      <td>2020-01-02T03:45:27.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>195.62</td>\n",
       "      <td>1.709517</td>\n",
       "      <td>1.709517</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>610a9e76bd6a3fb56bed7501b3ec6c0e</td>\n",
       "      <td>2020-01-02T07:14:02.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>653.63</td>\n",
       "      <td>2.622598</td>\n",
       "      <td>2.622598</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c24caf694d7b5375b4a72224c5351949</td>\n",
       "      <td>2020-01-02T16:43:26.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>20.18</td>\n",
       "      <td>0.099204</td>\n",
       "      <td>0.099204</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                tid                  datetime  fraud_label  \\\n",
       "0  9865906a3fc8ffb36edd7413302fd50d  2020-01-01T08:03:37.000Z            0   \n",
       "1  b18b52528c812800f93c815139480f1f  2020-01-01T11:23:16.000Z            0   \n",
       "2  34b9c71ea65c2a9003dd67e65e4507ec  2020-01-02T03:45:27.000Z            0   \n",
       "3  610a9e76bd6a3fb56bed7501b3ec6c0e  2020-01-02T07:14:02.000Z            0   \n",
       "4  c24caf694d7b5375b4a72224c5351949  2020-01-02T16:43:26.000Z            0   \n",
       "\n",
       "   amount  amt_ratio1  amt_ratio2  count_ratio  \n",
       "0   89.69    1.000000    1.000000     1.000000  \n",
       "1   57.98    0.785264    0.785264     0.500000  \n",
       "2  195.62    1.709517    1.709517     0.333333  \n",
       "3  653.63    2.622598    2.622598     0.250000  \n",
       "4   20.18    0.099204    0.099204     0.200000  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processing_out_df['datetime1'] = pd.to_datetime(processing_out_df['datetime'],format='%Y-%m-%dT%H:%M:%S.%fZ').astype(str)\n",
    "\n",
    "df1 = processing_out_df[['tid','datetime','fraud_label', 'amount', 'amt_ratio1','amt_ratio2','count_ratio']]\n",
    "\n",
    "df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tid             object\n",
       "datetime        object\n",
       "fraud_label      int64\n",
       "amount         float64\n",
       "amt_ratio1     float64\n",
       "amt_ratio2     float64\n",
       "count_ratio    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31440/1325488508.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['tid']=df1['tid'].astype('string')\n",
      "/tmp/ipykernel_31440/1325488508.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['datetime']=df1['datetime'].astype('string')\n"
     ]
    }
   ],
   "source": [
    "df1['tid']=df1['tid'].astype('string')\n",
    "df1['datetime']=df1['datetime'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tid             string\n",
       "datetime        string\n",
       "fraud_label      int64\n",
       "amount         float64\n",
       "amt_ratio1     float64\n",
       "amt_ratio2     float64\n",
       "count_ratio    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuregroup_name = 'cc-trans-fg'\n",
    "\n",
    "feature_group = FeatureGroup(name=featuregroup_name, sagemaker_session=feature_store_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tid             string\n",
       "datetime        string\n",
       "fraud_label      int64\n",
       "amount         float64\n",
       "amt_ratio1     float64\n",
       "amt_ratio2     float64\n",
       "count_ratio    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import gmtime, strftime, sleep\n",
    "current_timestamp = strftime('%m-%d-%H-%M', gmtime())\n",
    "\n",
    "sample_df = pd.DataFrame([['d621c8d794262ad5e8ad804cb4517395', '2023-03-01T00:00:00Z', 0,8911.09, 1.0,1.0,1.0]], \n",
    "                  columns=['tid', 'datetime', 'fraud_label', 'amount', 'amt_ratio1','amt_ratio2','count_ratio'])\n",
    "\n",
    "# sample_df.dtypes\n",
    "sample_df['tid']=sample_df['tid'].astype('string')\n",
    "sample_df['datetime']=sample_df['datetime'].astype('string')\n",
    "sample_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FeatureDefinition(feature_name='tid', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n",
       " FeatureDefinition(feature_name='datetime', feature_type=<FeatureTypeEnum.STRING: 'String'>),\n",
       " FeatureDefinition(feature_name='fraud_label', feature_type=<FeatureTypeEnum.INTEGRAL: 'Integral'>),\n",
       " FeatureDefinition(feature_name='amount', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='amt_ratio1', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='amt_ratio2', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>),\n",
       " FeatureDefinition(feature_name='count_ratio', feature_type=<FeatureTypeEnum.FRACTIONAL: 'Fractional'>)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group.load_feature_definitions(data_frame=sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location of offline store: s3://sm-fs-demo/sagemaker-feature-store\n"
     ]
    }
   ],
   "source": [
    "offline_feature_store_uri = f's3://sm-fs-demo/sagemaker-feature-store'\n",
    "\n",
    "print(f'Location of offline store: {offline_feature_store_uri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:461312420708:feature-group/cc-trans-fg',\n",
       " 'ResponseMetadata': {'RequestId': '1c60163f-567a-45d0-8f12-da4e3b5d0c34',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '1c60163f-567a-45d0-8f12-da4e3b5d0c34',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '88',\n",
       "   'date': 'Sun, 09 Apr 2023 19:50:33 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_group.create( s3_uri=offline_feature_store_uri, \n",
    "                               record_identifier_name='tid', \n",
    "                               event_time_feature_name='datetime', \n",
    "                               role_arn=sagemaker_role, \n",
    "                               enable_online_store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group.ingest(data_frame=df1, max_processes=16, wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
